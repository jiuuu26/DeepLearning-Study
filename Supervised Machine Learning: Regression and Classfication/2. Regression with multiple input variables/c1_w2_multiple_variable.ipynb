import copy, math
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(precision=2)    #소수점 이하 둘째 자리까지 표시
# 3개의 예시, 4개의 feature(size, bedrooms, floors and age)
X_train=np.array([[2104,5,1,45],[1416,3,2,40],[852,2,1,35]])
y_train=np.array([460,232,178])
m개의 예시, n개의 특징
X = (m,n) matrix
print(f"X shape : {X_train.shape}, X type : {type(X_train)}")
print(X_train)
print(f"y shape : {y_train.shape}, y type : {type(y_train)}")
print(y_train)
n개의 feature 만큼 w가 존재해야한다. 하나의 b 값도 존재해야한다. 여기서 w는 vector, b는 scalar이다. 
b_init=785.1811367994083
w_init=np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])
print(f"w_init shape : {w_init.shape}, b_init type : {type(b_init)}")
선형 회귀 모델을 위해서는 "dot product"를 실행해야한다. dot product를 실행하기 전에 직접 loop로 함수를 만들어 보자.
def predict_single_loop(x,w,b): #x에 뭐가 들어갈까?
    n=x.shape[0]    #첫번째 차원의 크기, 1차원 배열에서는 요소의 수를 나타냄 
    p=0
    for i in range(n):
        p_i=x[i]*w[i]
        p=p+p_i
    p=p+b
    return p
# 3개의 예시 중 첫번째 예시
x_vec = X_train[0,:]
print(f"x_vec shape : {x_vec.shape}, x_vec value : {x_vec}")

f_wb = predict_single_loop(x_vec, w_init, b_init)
print(f"f_wb shape : {f_wb.shape}, prediction : {f_wb}")

dot product로 predict하기
def predict(x,w,b):
    p=np.dot(x,w)+b
    return p
x_vec = X_train[0,:]
print(f"x_vec shape : {x_vec.shape}, x_vec value : {x_vec}")

f_wb = predict(x_vec, w_init, b_init)
print(f"f_wb shape : {f_wb.shape}, prediction : {f_wb}")
직접 for loop로 predict 한 것과 dot product로 predict 한 결과가 같은 것을 확인할 수 있다. 그렇기 때문에 앞으로는 더 간단한 np.dot을 이용할 것이다.  
def compute_cost(X,y,w,b):
    m=X.shape[0]
    cost=0.0
    for i in range(m):
        f_wb_i=np.dot(X[i],w)+b
        cost = cost + (f_wb_i-y[i])**2
    cost = cost/(2*m)
    return cost
cost=compute_cost(X_train, y_train, w_init, b_init)
print(f"Cost at optimal w : {cost}")
Gradient Descent With Multiple Variables
def compute_gradient(X,y,w,b):
    m,n=X.shape
    dj_dw=np.zeros((n,))
    dj_db=0.

    for i in range(m):
        err=(np.dot(X[i],w)+b) - y[i]
        for j in range(n):
            dj_dw[j]=dj_dw[j]+err*X[i,j]
        dj_db=dj_db+err
    dj_dw=dj_dw/m
    dj_db=dj_db/m

    return dj_db,dj_dw
tmp_dj_db, tmp_dj_dw = compute_gradient(X_train,y_train,w_init,b_init)
print(f"dj_db at initial w,b: {tmp_dj_db}")
print(f"dj_dw at initial w,b: \n {tmp_dj_dw}")
def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):
    J_history = []
    w=copy.deepcopy(w_in)
    b=b_in

    for i in range(num_iters):
        dj_db, dj_dw = gradient_function(X,y,w,b)
        w=w-alpha*dj_dw
        b=b-alpha*dj_db

        if i<100000:
            J_history.append(cost_function(X,y,w,b))

        if i % math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d} : Cost {J_history[-1]:8.2f}    ")
    return w,b,J_history
# w_init과 같은 shape의 행렬에 0의 값을 넣은 행렬 생성
initial_w = np.zeros_like(w_init)
initial_b = 0.

iterations=1000
alpha=5.0e-7
w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations)

print(f"b,w found by gradient descent: {b_final:0.2f}, {w_final}")
m,_ = X_train.shape
for i in range(m):
    print(f"prediction: {np.dot(X_train[i], w_final)+b_final:0.2f}, target value: {y_train[i]}")
fig, (ax1, ax2) = plt.subplots(1,2,constrained_layout=True, figsize=(12,4))
ax1.plot(J_hist)
ax2.plot(100+np.arange(len(J_hist[100:])), J_hist[100:])
ax1.set_title("Cost vs. Iteration")
ax2.set_title("Cost vs. Iteration (tail)")
ax1.set_ylabel('Cost')
ax2.set_ylabel('Cost')
ax1.set_xlabel('Iteration step')
ax2.set_xlabel('Iteration step')
plt.show()
